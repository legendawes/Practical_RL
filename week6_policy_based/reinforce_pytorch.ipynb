{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a lasagne network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa9557c5470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть два действия. Почему мы не будем использовать софтмакс?\n",
    "Потому что мы всё равно будем брать логарифм. В нашей оценке фигурирует производная логарифма вероятности.\n",
    "\n",
    "$\\pi(a_0|s) = logit_0 - log(e^{logit_0} + e^{logit_1})$\n",
    "\n",
    "В терминах pytorch последнее - F.log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. Keep it simple: CartPole isn't worth deep architectures.\n",
    "agent = nn.Sequential(nn.Linear(state_dim[0], 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, n_actions)\n",
    "                     )\n",
    "\n",
    "# < YOUR CODE HERE: define a neural network that predicts policy logits >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  6.7289  5.2992\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Variable(torch.FloatTensor(env.reset().reshape([1,4])))\n",
    "\n",
    "agent(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили два логита, вероятности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5036  0.4964\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(agent(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.6860 -0.7003\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(agent(ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda and torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor\n",
    "else:\n",
    "    from torch import FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    states = Variable(torch.FloatTensor(states))\n",
    "    \n",
    "    return F.softmax(agent(states)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba(np.array([s]))[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i.e G\n",
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_rews = []\n",
    "    \n",
    "    prev_reward = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        cum_rew = reward + prev_reward * gamma\n",
    "        cum_rews.append(cum_rew)\n",
    "        prev_reward = cum_rew\n",
    "        \n",
    "    return np.array(cum_rews[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40049, 1.5561 , 1.729  , 0.81   , 0.9    , 1.     , 0.     ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "opt = torch.optim.Adam(agent.parameters())\n",
    "def train_on_session(states, actions, rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast everything into a variable\n",
    "    states = Variable(torch.FloatTensor(states))\n",
    "    actions = Variable(torch.IntTensor(actions))\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = Variable(torch.FloatTensor(cumulative_returns))\n",
    "    \n",
    "    # predict logits, probas and log-probas using an agent. \n",
    "    logits = agent(states)\n",
    "    probas = F.softmax(logits)\n",
    "    logprobas = F.log_softmax(logits)\n",
    "    \n",
    "    assert all(isinstance(v, Variable) for v in [logits, probas, logprobas]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_proba function\"\n",
    "    \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    # only for those actions that agent actually did.\n",
    "    # We only influence probability of given action in a given state.\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim = 1)\n",
    "    \n",
    "    # REINFORCE objective function\n",
    "    # we dont want our gradient to depend on the length of the game\n",
    "    J_hat = torch.mean(logprobas_for_actions * cumulative_returns)\n",
    "    \n",
    "    #regularize with entropy\n",
    "    #it is not a binary log, but just a coef so whatever\n",
    "    entropy_reg = - (probas * logprobas).sum(-1).mean()\n",
    "    \n",
    "    loss = - J_hat - 0.1 * entropy_reg\n",
    "    \n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:23.110\n",
      "mean reward:32.230\n",
      "mean reward:50.470\n",
      "mean reward:63.980\n",
      "mean reward:109.150\n",
      "mean reward:108.990\n",
      "mean reward:156.370\n",
      "mean reward:150.740\n",
      "mean reward:261.870\n",
      "mean reward:290.720\n",
      "mean reward:173.780\n",
      "mean reward:404.080\n",
      "mean reward:427.050\n",
      "mean reward:239.520\n",
      "mean reward:520.310\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print (\"You Win!\") # but you can train even further\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qlearning wasn't improving score in the beginning, because it was learning a lot of things in the beginning\n",
    "\n",
    "Now we optimize straight from the beginning and everybody is happy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/igolovanov/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.23535.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
